WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[02/03/23 16:54:41] INFO     colossalai - colossalai - INFO:                                                                                
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:521 set_device    
[02/03/23 16:54:41] INFO     colossalai - colossalai - INFO:                                                                                
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:521 set_device    
                    INFO     colossalai - colossalai - INFO: process rank 3 is bound to device 3                                            
                    INFO     colossalai - colossalai - INFO: process rank 2 is bound to device 2                                            
[02/03/23 16:54:41] INFO     colossalai - colossalai - INFO:                                                                                
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:521 set_device    
                    INFO     colossalai - colossalai - INFO: process rank 0 is bound to device 0                                            
[02/03/23 16:54:41] INFO     colossalai - colossalai - INFO:                                                                                
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:521 set_device    
                    INFO     colossalai - colossalai - INFO: process rank 1 is bound to device 1                                            
[02/03/23 16:54:43] INFO     colossalai - colossalai - INFO:                                                                                
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:557 set_seed      
[02/03/23 16:54:43] INFO     colossalai - colossalai - INFO:                                                                                
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:557 set_seed      
                    INFO     colossalai - colossalai - INFO: initialized seed on rank 0, numpy: 1024, python random: 1024,                  
                             ParallelMode.DATA: 1024, ParallelMode.TENSOR: 1024,the default parallel seed is ParallelMode.DATA.             
                    INFO     colossalai - colossalai - INFO: initialized seed on rank 2, numpy: 1024, python random: 1024,                  
                             ParallelMode.DATA: 1024, ParallelMode.TENSOR: 1024,the default parallel seed is ParallelMode.DATA.             
[02/03/23 16:54:43] INFO     colossalai - colossalai - INFO:                                                                                
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:557 set_seed      
[02/03/23 16:54:43] INFO     colossalai - colossalai - INFO:                                                                                
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:557 set_seed      
                    INFO     colossalai - colossalai - INFO:                                                                                
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/initialize.py:117 launch                      
                    INFO     colossalai - colossalai - INFO: initialized seed on rank 1, numpy: 1024, python random: 1024,                  
                             ParallelMode.DATA: 1024, ParallelMode.TENSOR: 1024,the default parallel seed is ParallelMode.DATA.             
                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized, data parallel size: 4, pipeline        
                             parallel size: 1, tensor parallel size: 1                                                                      
                    INFO     colossalai - colossalai - INFO: initialized seed on rank 3, numpy: 1024, python random: 1024,                  
                             ParallelMode.DATA: 1024, ParallelMode.TENSOR: 1024,the default parallel seed is ParallelMode.DATA.             
[02/03/23 16:54:45] INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:313 main                                              
                    INFO     colossalai - colossalai - INFO: Model config has been created                                                  
[02/03/23 16:54:46] INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:319 main                                              
                    INFO     colossalai - colossalai - INFO: GPT2Tokenizer has been created                                                 
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:335 main                                              
                    INFO     colossalai - colossalai - INFO: Finetune a pre-trained model                                                   
[02/03/23 16:54:50] INFO     colossalai - ProcessGroup - INFO:                                                                              
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/process_group.py:24 get                
                    INFO     colossalai - ProcessGroup - INFO: NCCL initialize ProcessGroup on [0]                                          
                    INFO     colossalai - ProcessGroup - INFO:                                                                              
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/process_group.py:24 get                
                    INFO     colossalai - ProcessGroup - INFO: NCCL initialize ProcessGroup on [1]                                          
                    INFO     colossalai - ProcessGroup - INFO:                                                                              
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/process_group.py:24 get                
                    INFO     colossalai - ProcessGroup - INFO: NCCL initialize ProcessGroup on [2]                                          
                    INFO     colossalai - ProcessGroup - INFO:                                                                              
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/process_group.py:24 get                
                    INFO     colossalai - ProcessGroup - INFO: NCCL initialize ProcessGroup on [3]                                          
                    INFO     colossalai - ProcessGroup - INFO:                                                                              
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/process_group.py:24 get                
                    INFO     colossalai - ProcessGroup - INFO: NCCL initialize ProcessGroup on [0, 1, 2, 3]                                 
searching chunk configuration is completed in 1.52 s.
used number: 2528.76 MB, wasted number: 32.74 MB
total wasted percentage is 1.28%
/root/anaconda3/envs/py38/lib/python3.8/site-packages/datasets/dataset_dict.py:1241: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.
You can remove this warning by passing 'storage_options=fs.storage_options' instead.
  warnings.warn(
/root/anaconda3/envs/py38/lib/python3.8/site-packages/datasets/dataset_dict.py:1241: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.
You can remove this warning by passing 'storage_options=fs.storage_options' instead.
  warnings.warn(
/root/anaconda3/envs/py38/lib/python3.8/site-packages/datasets/dataset_dict.py:1241: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.
You can remove this warning by passing 'storage_options=fs.storage_options' instead.
  warnings.warn(
[02/03/23 16:55:24] INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:348 main                                              
                    INFO     colossalai - colossalai - INFO: GeminiDDP has been created                                                     
/root/anaconda3/envs/py38/lib/python3.8/site-packages/datasets/dataset_dict.py:1241: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.
You can remove this warning by passing 'storage_options=fs.storage_options' instead.
  warnings.warn(
Traceback (most recent call last):
  File "train_gemini_opt_0203.py", line 488, in <module>
    main()
  File "train_gemini_opt_0203.py", line 433, in main
    for step, batch in enumerate(train_dataloader):
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 681, in __next__
    data = self._next_data()
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 721, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    return self.collate_fn(data)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/data/data_collator.py", line 70, in default_data_collator
    return torch_default_data_collator(features)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/data/data_collator.py", line 136, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
ValueError: expected sequence of length 114 at dim 1 (got 121)
[02/03/23 16:55:25] INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:358 main                                              
                    INFO     colossalai - colossalai - INFO: Dataset is prepared                                                            
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:373 main                                              
                    INFO     colossalai - colossalai - INFO: Dataloaders have been created                                                  
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:413 main                                              
                    INFO     colossalai - colossalai - INFO: ***** Running training *****                                                   
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:414 main                                              
                    INFO     colossalai - colossalai - INFO:   Num examples = 6164188                                                       
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:415 main                                              
                    INFO     colossalai - colossalai - INFO:   Num Epochs = 3                                                               
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:416 main                                              
                    INFO     colossalai - colossalai - INFO:   Instantaneous batch size per device = 16                                     
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:417 main                                              
                    INFO     colossalai - colossalai - INFO:   Total train batch size (w. parallel, distributed & accumulation) = 64        
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:418 main                                              
                    INFO     colossalai - colossalai - INFO:   Gradient Accumulation steps = 1                                              
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:419 main                                              
                    INFO     colossalai - colossalai - INFO:   Total optimization steps = 288948                                            
  0%|          | 0/288948 [00:00<?, ?it/s]Traceback (most recent call last):
  File "train_gemini_opt_0203.py", line 488, in <module>
    main()
  File "train_gemini_opt_0203.py", line 433, in main
    for step, batch in enumerate(train_dataloader):
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 681, in __next__
    data = self._next_data()
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 721, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    return self.collate_fn(data)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/data/data_collator.py", line 70, in default_data_collator
    return torch_default_data_collator(features)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/data/data_collator.py", line 136, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
ValueError: expected sequence of length 256 at dim 1 (got 194)
Traceback (most recent call last):
  File "train_gemini_opt_0203.py", line 488, in <module>
    main()
  File "train_gemini_opt_0203.py", line 433, in main
    for step, batch in enumerate(train_dataloader):
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 681, in __next__
    data = self._next_data()
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 721, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    return self.collate_fn(data)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/data/data_collator.py", line 70, in default_data_collator
    return torch_default_data_collator(features)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/data/data_collator.py", line 136, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
ValueError: expected sequence of length 360 at dim 1 (got 1024)
Traceback (most recent call last):
  File "train_gemini_opt_0203.py", line 488, in <module>
    main()
  File "train_gemini_opt_0203.py", line 433, in main
    for step, batch in enumerate(train_dataloader):
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 681, in __next__
    data = self._next_data()
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 721, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    return self.collate_fn(data)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/data/data_collator.py", line 70, in default_data_collator
    return torch_default_data_collator(features)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/data/data_collator.py", line 136, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
ValueError: expected sequence of length 118 at dim 1 (got 109)
  0%|          | 0/288948 [00:00<?, ?it/s]
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 13005 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 13006 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 13007 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 3 (pid: 13010) of binary: /root/anaconda3/envs/py38/bin/python
Traceback (most recent call last):
  File "/root/anaconda3/envs/py38/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/run.py", line 761, in main
    run(args)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_gemini_opt_0203.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-02-03_16:55:28
  host      : 82bd6a1f9bbe
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 13010)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
