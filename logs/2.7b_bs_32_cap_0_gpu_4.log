WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[02/04/23 23:14:37] INFO     colossalai - colossalai - INFO: /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:521 set_device     
                    INFO     colossalai - colossalai - INFO: process rank 1 is bound to device 1                                                                             
[02/04/23 23:14:37] INFO     colossalai - colossalai - INFO: /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:521 set_device     
                    INFO     colossalai - colossalai - INFO: process rank 0 is bound to device 0                                                                             
[02/04/23 23:14:37] INFO     colossalai - colossalai - INFO: /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:521 set_device     
                    INFO     colossalai - colossalai - INFO: process rank 3 is bound to device 3                                                                             
[02/04/23 23:14:37] INFO     colossalai - colossalai - INFO: /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:521 set_device     
                    INFO     colossalai - colossalai - INFO: process rank 2 is bound to device 2                                                                             
[02/04/23 23:14:40] INFO     colossalai - colossalai - INFO: /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:557 set_seed       
[02/04/23 23:14:40] INFO     colossalai - colossalai - INFO: /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:557 set_seed       
[02/04/23 23:14:40] INFO     colossalai - colossalai - INFO: /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:557 set_seed       
                    INFO     colossalai - colossalai - INFO: initialized seed on rank 2, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR:     
                             1024,the default parallel seed is ParallelMode.DATA.                                                                                            
                    INFO     colossalai - colossalai - INFO: initialized seed on rank 0, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR:     
                             1024,the default parallel seed is ParallelMode.DATA.                                                                                            
[02/04/23 23:14:40] INFO     colossalai - colossalai - INFO: /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/context/parallel_context.py:557 set_seed       
                    INFO     colossalai - colossalai - INFO: initialized seed on rank 3, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR:     
                             1024,the default parallel seed is ParallelMode.DATA.                                                                                            
                    INFO     colossalai - colossalai - INFO: /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/initialize.py:117 launch                       
                    INFO     colossalai - colossalai - INFO: initialized seed on rank 1, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR:     
                             1024,the default parallel seed is ParallelMode.DATA.                                                                                            
                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized, data parallel size: 4, pipeline parallel size: 1, tensor parallel size: 
                             1                                                                                                                                               
[02/04/23 23:14:41] INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:319 main                                                        
                    INFO     colossalai - colossalai - INFO: Model config has been created                                                            
[02/04/23 23:14:43] INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:325 main                                                        
                    INFO     colossalai - colossalai - INFO: GPT2Tokenizer has been created                                                           
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:341 main                                                        
                    INFO     colossalai - colossalai - INFO: Finetune a pre-trained model                                                             
[02/04/23 23:14:47] INFO     colossalai - ProcessGroup - INFO:                                                                                        
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/process_group.py:24 get                          
                    INFO     colossalai - ProcessGroup - INFO: NCCL initialize ProcessGroup on [0]                                                    
                    INFO     colossalai - ProcessGroup - INFO:                                                                                        
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/process_group.py:24 get                          
                    INFO     colossalai - ProcessGroup - INFO: NCCL initialize ProcessGroup on [1]                                                    
                    INFO     colossalai - ProcessGroup - INFO:                                                                                        
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/process_group.py:24 get                          
                    INFO     colossalai - ProcessGroup - INFO: NCCL initialize ProcessGroup on [2]                                                    
                    INFO     colossalai - ProcessGroup - INFO:                                                                                        
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/process_group.py:24 get                          
                    INFO     colossalai - ProcessGroup - INFO: NCCL initialize ProcessGroup on [3]                                                    
                    INFO     colossalai - ProcessGroup - INFO:                                                                                        
                             /root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/process_group.py:24 get                          
                    INFO     colossalai - ProcessGroup - INFO: NCCL initialize ProcessGroup on [0, 1, 2, 3]                                           
searching chunk configuration is completed in 1.59 s.
used number: 2528.76 MB, wasted number: 32.74 MB
total wasted percentage is 1.28%
[02/04/23 23:15:21] INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:354 main                                                        
                    INFO     colossalai - colossalai - INFO: GeminiDDP has been created                                                               
[02/04/23 23:17:09] INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:364 main                                              
                    INFO     colossalai - colossalai - INFO: Dataset is prepared                                                            
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:379 main                                              
                    INFO     colossalai - colossalai - INFO: Dataloaders have been created                                                  
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:407 main                                              
                    INFO     colossalai - colossalai - INFO: ***** Running training *****                                                   
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:408 main                                              
                    INFO     colossalai - colossalai - INFO:   Num examples = 6164188                                                       
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:409 main                                              
                    INFO     colossalai - colossalai - INFO:   Num Epochs = 3                                                               
[02/04/23 23:17:33] INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:410 main                                              
                    INFO     colossalai - colossalai - INFO:   Instantaneous batch size per device = 32                                     
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:411 main                                              
                    INFO     colossalai - colossalai - INFO:   Total train batch size (w. parallel, distributed & accumulation) = 128       
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:412 main                                              
                    INFO     colossalai - colossalai - INFO:   Gradient Accumulation steps = 1                                              
                    INFO     colossalai - colossalai - INFO: train_gemini_opt_0203.py:413 main                                              
                    INFO     colossalai - colossalai - INFO:   Total optimization steps = 144474                                            
  0%|          | 0/144474 [00:00<?, ?it/s]Traceback (most recent call last):
  File "train_gemini_opt_0203.py", line 482, in <module>
    main()
  File "train_gemini_opt_0203.py", line 433, in main
    optimizer.backward(loss)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/nn/optimizer/zero_optimizer.py", line 225, in backward
    self.module.backward(loss)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/nn/parallel/data_parallel.py", line 297, in backward
    loss.backward()
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py", line 388, in backward
Traceback (most recent call last):
  File "train_gemini_opt_0203.py", line 482, in <module>
    main()
  File "train_gemini_opt_0203.py", line 433, in main
    optimizer.backward(loss)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/nn/optimizer/zero_optimizer.py", line 225, in backward
    self.module.backward(loss)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/nn/parallel/data_parallel.py", line 297, in backward
    loss.backward()
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py", line 388, in backward
Traceback (most recent call last):
  File "train_gemini_opt_0203.py", line 482, in <module>
    main()
  File "train_gemini_opt_0203.py", line 433, in main
    optimizer.backward(loss)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/nn/optimizer/zero_optimizer.py", line 225, in backward
    self.module.backward(loss)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/nn/parallel/data_parallel.py", line 297, in backward
    loss.backward()
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py", line 388, in backward
    return handle_torch_function(
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/overrides.py", line 1498, in handle_torch_function
    return handle_torch_function(
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/overrides.py", line 1498, in handle_torch_function
    return handle_torch_function(
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/overrides.py", line 1498, in handle_torch_function
    result = torch_func_method(public_api, types, args, kwargs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/colo_tensor.py", line 180, in __torch_function__
    result = torch_func_method(public_api, types, args, kwargs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/colo_tensor.py", line 180, in __torch_function__
    result = torch_func_method(public_api, types, args, kwargs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/colo_tensor.py", line 180, in __torch_function__
    return backward_tensor.backward(**tensor_kwargs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    return backward_tensor.backward(**tensor_kwargs)
      File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
return backward_tensor.backward(**tensor_kwargs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/function.py", line 253, in apply
        Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward passVariable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/function.py", line 253, in apply
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 146, in backward
    return user_fn(self, *args)
    return user_fn(self, *args)  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 146, in backward

  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 146, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 2; 23.70 GiB total capacity; 17.61 GiB already allocated; 1.52 GiB free; 21.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
RuntimeError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 23.70 GiB total capacity; 17.61 GiB already allocated; 1.52 GiB free; 21.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
RuntimeError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 1; 23.70 GiB total capacity; 17.61 GiB already allocated; 1.52 GiB free; 21.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_gemini_opt_0203.py", line 482, in <module>
    main()
  File "train_gemini_opt_0203.py", line 433, in main
    optimizer.backward(loss)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/nn/optimizer/zero_optimizer.py", line 225, in backward
    self.module.backward(loss)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/nn/parallel/data_parallel.py", line 297, in backward
    loss.backward()
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py", line 388, in backward
    return handle_torch_function(
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/overrides.py", line 1498, in handle_torch_function
    result = torch_func_method(public_api, types, args, kwargs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/colossalai/tensor/colo_tensor.py", line 180, in __torch_function__
    return backward_tensor.backward(**tensor_kwargs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 146, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 3; 23.70 GiB total capacity; 17.61 GiB already allocated; 1.52 GiB free; 21.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  0%|          | 0/144474 [01:04<?, ?it/s]
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 116058 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 116059) of binary: /root/anaconda3/envs/py38/bin/python
Traceback (most recent call last):
  File "/root/anaconda3/envs/py38/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/run.py", line 761, in main
    run(args)
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_gemini_opt_0203.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-02-04_23:18:43
  host      : 82bd6a1f9bbe
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 116060)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-02-04_23:18:43
  host      : 82bd6a1f9bbe
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 116061)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-02-04_23:18:43
  host      : 82bd6a1f9bbe
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 116059)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
